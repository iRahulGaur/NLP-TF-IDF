{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aec6550-a3f4-4c48-b729-9299932763c9",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "In this notebook we will learn tokenization. There are 4 different termnologies we use in tokenization.\n",
    " 1. **Corpus** - This is a paragraph\n",
    " 2. **Documents** - This is all the sentences from *corpus*\n",
    " 3. **Vocabulary** - This is all the unique words available in the *corpus*\n",
    " 4. **Words** - This is all the words available in the *corpus*.\n",
    "\n",
    "In this notebook we will handle all the different aspects of tokenization from documents to words. we will be using **NLTK** library to perform tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5adb9fbd-e0a1-4363-8282-0400aff9a24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\rahul\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "739a0fe0-0031-4cff-bf87-5d93fd710b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7600f313-cff9-448d-aef7-e11134e7fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"Natural Language Processing is a fascinating field of AI.\n",
    "Tokenization is the first step in NLP preprocessing.\n",
    "Python provides various libraries for NLP, such as NLTK and spaCy.\n",
    "Text data needs to be cleaned before applying machine learning models.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "282fa128-c503-4b0b-acbd-486dc73622a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing is a fascinating field of AI.\n",
      "Tokenization is the first step in NLP preprocessing.\n",
      "Python provides various libraries for NLP, such as NLTK and spaCy.\n",
      "Text data needs to be cleaned before applying machine learning models.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1695866-af6f-42c4-923e-215f52069141",
   "metadata": {},
   "source": [
    "## Sentence --> paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "842cd764-03c7-4c86-9fce-9d4d3e6523e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55624430-b3a7-4554-8347-9ce314771933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = sent_tokenize(corpus)\n",
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffed6e47-4881-4e2a-9648-37e65105bc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing is a fascinating field of AI.\n",
      "Tokenization is the first step in NLP preprocessing.\n",
      "Python provides various libraries for NLP, such as NLTK and spaCy.\n",
      "Text data needs to be cleaned before applying machine learning models.\n"
     ]
    }
   ],
   "source": [
    "for document  in documents:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4271ba2-9f7f-4cde-87b1-bd03edf4c4df",
   "metadata": {},
   "source": [
    "## Paragraph --> words\n",
    "## sentence --> words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d2ea2c1-dd02-4a48-89a5-8df27e513682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2329bfa7-2d2e-4886-a28d-52d32810bb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'of', 'AI', '.', 'Tokenization', 'is', 'the', 'first', 'step', 'in', 'NLP', 'preprocessing', '.', 'Python', 'provides', 'various', 'libraries', 'for', 'NLP', ',', 'such', 'as', 'NLTK', 'and', 'spaCy', '.', 'Text', 'data', 'needs', 'to', 'be', 'cleaned', 'before', 'applying', 'machine', 'learning', 'models', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(corpus)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "949f77d9-1888-4f9b-9a26-5d38c4ae8e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'of', 'AI', '.']\n",
      "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'NLP', 'preprocessing', '.']\n",
      "['Python', 'provides', 'various', 'libraries', 'for', 'NLP', ',', 'such', 'as', 'NLTK', 'and', 'spaCy', '.']\n",
      "['Text', 'data', 'needs', 'to', 'be', 'cleaned', 'before', 'applying', 'machine', 'learning', 'models', '.']\n"
     ]
    }
   ],
   "source": [
    "for document  in documents:\n",
    "    print(word_tokenize(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9639f1a0-59c6-4085-a354-dad384c78a4b",
   "metadata": {},
   "source": [
    "---\n",
    "Below we are using a different library from NLTK -> *wordpunct_tokenize*.\n",
    "\n",
    "This library will also include punctuation in tokenization as observed in the below output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5fa35c2-ac2e-4280-b25d-ea6654ebbbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9be060c7-7f4c-4ba7-b0b4-6c283781a01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'of', 'AI', '.', 'Tokenization', 'is', 'the', 'first', 'step', 'in', 'NLP', 'preprocessing', '.', 'Python', 'provides', 'various', 'libraries', 'for', 'NLP', ',', 'such', 'as', 'NLTK', 'and', 'spaCy', '.', 'Text', 'data', 'needs', 'to', 'be', 'cleaned', 'before', 'applying', 'machine', 'learning', 'models', '.']\n"
     ]
    }
   ],
   "source": [
    "wordsPunct = word_tokenize(corpus)\n",
    "print(wordsPunct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566a1388-fe67-4c0f-bee5-c83a3fa63f86",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can also use *TreebankWordTokenizer* which will not consider \".\" as a seperate token but only the last \".\" (full stop) will be treated as a seperate token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f048ad78-4d70-458e-92a0-f514227f057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46ccf426-6c19-43b7-958b-4df460d5111a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'is',\n",
       " 'a',\n",
       " 'fascinating',\n",
       " 'field',\n",
       " 'of',\n",
       " 'AI.',\n",
       " 'Tokenization',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'step',\n",
       " 'in',\n",
       " 'NLP',\n",
       " 'preprocessing.',\n",
       " 'Python',\n",
       " 'provides',\n",
       " 'various',\n",
       " 'libraries',\n",
       " 'for',\n",
       " 'NLP',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'NLTK',\n",
       " 'and',\n",
       " 'spaCy.',\n",
       " 'Text',\n",
       " 'data',\n",
       " 'needs',\n",
       " 'to',\n",
       " 'be',\n",
       " 'cleaned',\n",
       " 'before',\n",
       " 'applying',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'models',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ddff0-0ae3-4147-a7e6-76a23eb0714b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
